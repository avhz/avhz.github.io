<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Gradient Descent</title>

<script src="site_libs/header-attrs-2.14/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>






<link rel="stylesheet" href="styles.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html"><p><span class="math inline">\(f(\alpha\zeta)\)</span></p></a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Programming
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="rcpp_index.html">Rcpp</a>
    </li>
    <li>
      <a href="gradient_descent.html">Gradient Descent</a>
    </li>
    <li>
      <a href="MC_integration.html">Monte-Carlo Integration</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Mathematics
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="brownian_motion.html">Brownian Motion</a>
    </li>
    <li>
      <a href="MLE.html">Maximum Likelihood Estimation</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Trading
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="price_impact.html">Price Impact</a>
    </li>
  </ul>
</li>
<li>
  <a href="about.html">About</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Gradient Descent</h1>

</div>


<div id="thank-you-to-the-lovely-luna-lisa-rigby-heartsuit."
class="section level3">
<h3>Thank you to the lovely Luna Lisa Rigby <span
class="math inline">\(\heartsuit\)</span>.</h3>
</div>
<div id="the-steepest-descent-method." class="section level1">
<h1>The Steepest Descent Method.</h1>
<p>We want to implement an algorithm for solving uncostrained
optimisation problems of the form:</p>
<p><span class="math display">\[
\min_{x \in \mathbb{R}^n} f(x) \qquad f(x) \in \mathcal{C}^1
\]</span></p>
<p>when the objective function <span class="math inline">\(f(x)\)</span>
and its gradient, <span class="math inline">\(\nabla f(x)\)</span>, are
known.</p>
<p>We start with an initial guess, <span
class="math inline">\(x_0\)</span>, and perform the iteration:</p>
<p><span class="math display">\[
x_{k+1} = x_k + \alpha_k d_k = x_k - \alpha_k \nabla f(x_k)
\]</span></p>
<p>Where:</p>
<p><span class="math display">\[
\begin{aligned}
&amp; d_k = -\nabla f(x_k) &amp;&amp; \text{is the descent direction} \\
&amp;\alpha_k &amp;&amp; \text{is the step size in iteration $k$} \\
\end{aligned}
\]</span></p>
<p>This iteration gives us a monotonic sequence which converges to a
local minimum, <span class="math inline">\(f(x^*)\)</span>, if it
exists:</p>
<p><span class="math display">\[
f(x_0) \geq f(x_1) \geq f(x_2) \geq \cdots \geq f(x^*)
\]</span></p>
<p>The algorithm is repeated until the stationarity condition is
fulfilled:</p>
<p><span class="math display">\[
\nabla f(x) = 0
\]</span></p>
<p>Numerically, this condition is fulfilled if:</p>
<p><span class="math display">\[
\| \nabla f(x_{k+1}) \| \leq \epsilon
\]</span></p>
<p>Where <span class="math inline">\(\|\cdot\|\)</span> denotes the
Euclidean norm:</p>
<p><span class="math display">\[
\|x\| = \sqrt{\langle x,x \rangle}
\]</span></p>
<!-- ----------------------------------------------------------------------- -->
<div style="page-break-after: always;"></div>
<!-- ----------------------------------------------------------------------- -->
</div>
<div id="the-r-implementation." class="section level1">
<h1>The <code>R</code> Implementation.</h1>
<p>The function <code>steepestDescent()</code> takes the following
arguments:</p>
<ul>
<li><code>f</code>: objective function to minimise.</li>
<li><code>g</code>: gradient of the objective function.</li>
<li><code>x0</code>: initial point to begin the gradient descent.</li>
<li><code>stepsize</code>: <span class="math inline">\(\alpha\)</span>,
the step size or learning rate.
<ul>
<li><code>fixed.stepsize</code>: boolean indicating whether to use
optimal stepsize method.</li>
<li><code>max.stepsize</code>: the maximum stepsize allowed.</li>
</ul></li>
<li><code>iterations</code>: maximum number of iterations to perform, to
prevent infinite runtime for non-convergent functions.</li>
<li><code>tol</code>: numerical tolerance level for checking
stationarity (default is the square-root of the local machine
epsilon).</li>
</ul>
<p>We include the following libraries:</p>
<ul>
<li><code>lattice</code>: for plotting the objective functions.</li>
<li><code>compiler</code>: to byte-compile the function to improve
efficiency.</li>
</ul>
<pre class="r"><code>library(lattice)
library(compiler); enableJIT(3) # Enable Just-In-Time compiling.</code></pre>
<pre><code>## [1] 3</code></pre>
<!-- ----------------------------------------------------------------------- -->
<!-- ----------------------------------------------------------------------- -->
</div>
<div id="optimal-stepsize-alpha" class="section level1">
<h1>Optimal stepsize (<span class="math inline">\(\alpha\)</span>)</h1>
<p>For the <code>stepsize</code>, we can either use a fixed <span
class="math inline">\(\alpha\)</span>, or we can find an optimal <span
class="math inline">\(\alpha\)</span> based on the following <em>Limited
Minimisation Rule</em> algorithm:</p>
<ul>
<li>We take <span class="math inline">\(\alpha_k\)</span> such that the
function <span class="math inline">\(\alpha \mapsto f(x_k + \alpha
d_k)\)</span> is minimised over the interval <span
class="math inline">\(\alpha \in [0,s]\)</span>:</li>
</ul>
<p><span class="math display">\[
f(x_k + \alpha_k d_k) = \min_{\alpha \in [0,s]} f(x_k + \alpha d_k)
\]</span></p>
<p>This generally requires <span class="math inline">\(\alpha\)</span>
to be found numerically, thus we use the base <code>R</code> function
<code>optimize()</code>.</p>
<p>Using the optimal stepsize method, we find that in most cases it
dramatically decreases the number of iterations required to converge to
the minimum.</p>
<p>When using the optimal stepsize, by the <em>Zig-Zag Theorem</em>, for
all <span class="math inline">\(k\)</span>:</p>
<p><span class="math display">\[
\langle x_{k+1} - x_{k}, x_{k+2} - x_{k+1}\rangle = 0
\]</span></p>
<!-- ----------------------------------------------------------------------- -->
<!-- ----------------------------------------------------------------------- -->
</div>
<div id="testing-the-algorithm." class="section level1">
<h1>Testing the algorithm.</h1>
<p>In the case of a convex function the algorithm will return the
optimal minimiser given any finite starting value. This may not be the
case for functions with multiple local minima, and the choice of initial
starting point will determine the minima.</p>
<p>A way around this, or at least to reduce the chance of finding a
non-optimal minima, is the colloquially termed “shotgun” gradient
descent, which essentially means running the algorithm a number of times
with different starting points such that the optimal minima is more
likely to be found.</p>
<!-- ----------------------------------------------------------------------- -->
<div style="page-break-after: always;"></div>
<!-- ----------------------------------------------------------------------- -->
<pre class="r"><code>steepestDescent &lt;- function(
    
    # FUNCTION ARGUMENTS: ------------------------------------------------------
    f, g, x0,
    fixed.stepsize = TRUE,
    max.stepsize = 1.0,
    stepsize = 0.01,
    iterations = 10000,
    tol = sqrt(.Machine$double.eps)) {

    # WARNINGS/ERRORS: ---------------------------------------------------------
    
    if ( !is.numeric(x0) )  stop(&quot;Initial point vector must be numeric.&quot;)
    if ( tol &lt;= 0 )         stop(&quot;Tolerance must be positive.&quot;)
    if ( !is.function(f) )  stop(&quot;Objective function not provided.&quot;)
    if ( !is.function(g) )  stop(&quot;Gradient not provided.&quot;)
    
    # LOCAL FUNCTIONS: ---------------------------------------------------------
    
    isStationary &lt;- function(x, tol) return( sqrt(sum(x*x)) &lt; tol )

    # ASSIGN FUNCTIONS: --------------------------------------------------------
    
    f &lt;- match.fun(f)
    g &lt;- match.fun(g) 

    # PRE-ALLOCATE STORAGE; INITIALISE A COUNTER: ------------------------------
    
    n       &lt;- length(x0)
    k       &lt;- 1
    X       &lt;- matrix(NA, nrow = n, ncol = iterations)
    X[,k]   &lt;- x0
    grad    &lt;- g(X[,k])
    
    # PERFORM ITERATIVE GRADIENT DESCENT: --------------------------------------
    
    # Optimal stepsize: --------------------------------------------------------
    if( fixed.stepsize == FALSE ) {
        while ((k &lt; iterations) &amp;&amp; !isStationary(g(X[,k]), tol)) {
            stepsize    &lt;- optimize( function(a) f( X[,k] - a * grad ),
                                     interval = c(0, max.stepsize) )$min
            grad        &lt;- g(X[,k])
            X[,k+1]     &lt;- X[,k] - stepsize * grad
            k           &lt;- k + 1
        }
        X &lt;- X[,1:k]
    }
    # Fixed stepsize: ----------------------------------------------------------
    else {
        while ((k &lt; iterations) &amp;&amp; !isStationary(g(X[,k]), tol)) {
            grad        &lt;- g(X[,k])
            X[,k+1]     &lt;- X[,k] - stepsize * grad
            k           &lt;- k + 1
        }
        X &lt;- X[,1:k]
    }
    
    ifelse( n == 1,
            return( list(minimum = X[k], points = X, iterations = k) ),
            return( list(minimum = X[,k], points = X, iterations = k) ))
}

# BYTE-COMPILE THE FUNCTION: ---------------------------------------------------
steepestDescent &lt;- compiler::cmpfun(steepestDescent)</code></pre>
<p>In the following we will feed the algorithm different functions and
gradually increase the complexity of the task. We will test famous
functions such as the Rosenbrock; Rastrigin and Himmelblau’s
function.</p>
<!-- ----------------------------------------------------------------------- -->
<div style="page-break-after: always;"></div>
<!-- ----------------------------------------------------------------------- -->
</div>
<div id="function-1-univariate-convex-function." class="section level1">
<h1>Function 1: Univariate, convex function.</h1>
<p><span class="math display">\[
f(x) = (x-5)^2
\]</span> <span class="math display">\[
\text{Strict global minimum at: } x = 5\\
\]</span></p>
<pre class="r"><code>FUNC &lt;- function(x) return( (x - 5)^2 )
GRAD &lt;- function(x) return( 2 * (x - 5) )
x0   &lt;- 10

MIN1 &lt;- steepestDescent(FUNC, GRAD, x0 = x0, stepsize = 0.05)
MIN2 &lt;- steepestDescent(FUNC, GRAD, x0 = x0, fixed.stepsize = F)

MIN1[c(1,3)] # Global</code></pre>
<pre><code>## $minimum
## [1] 5
## 
## $iterations
## [1] 194</code></pre>
<pre class="r"><code>MIN2[c(1,3)] # Global</code></pre>
<pre><code>## $minimum
## [1] 5
## 
## $iterations
## [1] 2</code></pre>
<div id="function-1-plots." class="section level2">
<h2>Function 1: Plots.</h2>
<p><img src="gradient_descent_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<!-- ----------------------------------------------------------------------- -->
<div style="page-break-after: always;"></div>
<!-- ----------------------------------------------------------------------- -->
</div>
</div>
<div id="function-2-multivariate-convex-function."
class="section level1">
<h1>Function 2: Multivariate, convex function.</h1>
<p><span class="math display">\[
f(x,y) = x^2 + \frac{y^2}{3} - 5
\]</span></p>
<p><span class="math display">\[
\text{Strict global minimum at: } (x,y) = (0,0) \\
\]</span></p>
<pre class="r"><code>FUNC &lt;- function(x) {return(x[1]^2 + 1/3*x[2]^2 - 5)}
GRAD &lt;- function(x) {return(2*x[1] + 2/3*x[2])}
x0 &lt;- c(-1.9,-1.9)

MIN1 &lt;- steepestDescent(FUNC, GRAD, x0=x0, stepsize = 0.001)
MIN2 &lt;- steepestDescent(FUNC, GRAD, x0=x0, fixed.stepsize = F)

MIN1[c(1,3)] # correct</code></pre>
<pre><code>## $minimum
## [1] -5.585915e-09 -5.585915e-09
## 
## $iterations
## [1] 7358</code></pre>
<pre class="r"><code>MIN2[c(1,3)] # correct</code></pre>
<pre><code>## $minimum
## [1] -2.220446e-16 -2.220446e-16
## 
## $iterations
## [1] 2</code></pre>
<div id="function-2-plots." class="section level2">
<h2>Function 2: Plots.</h2>
<p><img src="gradient_descent_files/figure-html/unnamed-chunk-6-1.png" width="672" style="display: block; margin: auto;" /></p>
<p><img src="gradient_descent_files/figure-html/unnamed-chunk-7-1.png" width="50%" /><img src="gradient_descent_files/figure-html/unnamed-chunk-7-2.png" width="50%" /></p>
<!-- ----------------------------------------------------------------------- -->
<div style="page-break-after: always;"></div>
<!-- ----------------------------------------------------------------------- -->
</div>
</div>
<div id="function-3---multivariate-convex-function."
class="section level1">
<h1>Function 3 - Multivariate, convex function.</h1>
<p><span class="math display">\[
f(x,y,z) = x^2 + 2y^2 + 3z^2 + 2xy + 2xz
\]</span></p>
<p><span class="math display">\[
\text{Minimum at: } (x,y,z) = (0,0,0)
\]</span></p>
<pre class="r"><code>FUNC &lt;- function(x) {
  x[1]^2 + 2*x[2]^2 + 3*x[3]^2 + 2*x[1]*x[2] + 2*x[1]*x[3]
}

GRAD &lt;- function(x) {
  c(2*x[1] + 2*x[2] + 2*x[3], 
    4*x[2] + 2*x[1], 
    6*x[3] + 2*x[1])
}

x0   &lt;- c(1,2,3)
    
steepestDescent(FUNC, GRAD, x0 = x0)[c(1,3)]</code></pre>
<pre><code>## $minimum
## [1] -5.208267e-08  2.771261e-08  1.808812e-08
## 
## $iterations
## [1] 6846</code></pre>
<div id="function-3-plots." class="section level2">
<h2>Function 3: Plots.</h2>
<p>We are unable to plot 4-dimensional plots and can thus not illustrate
this case visually.</p>
<!-- ----------------------------------------------------------------------- -->
<div style="page-break-after: always;"></div>
<!-- ----------------------------------------------------------------------- -->
</div>
</div>
<div id="function-4-multivariate-non-convex-function."
class="section level1">
<h1>Function 4: Multivariate, non-convex function.</h1>
<p><span class="math display">\[
f(x,y) = \frac{x^4}{200} - \frac{x^3}{10} +3x - xy+y^2
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
\text{Global minimum: } (x,y) &amp;\approx (15.9771, 7.98856) \\
\text{Local minimum:  } (x,y) &amp;\approx (-3.59132, -1.79566) \\
\end{aligned}
\]</span></p>
<pre class="r"><code>FUNC &lt;- function(x) {
    x[1]^4/200 - x[1]^3/10 + 3*x[1] - x[1]*x[2] + x[2]^2
}

GRAD &lt;- function(x) {
    c(4*x[1]^3/200 - 3*x[1]^2/10 + 3 - x[2],
      - x[1] + 2*x[2])
}

# Fixed stepsize
MIN1 &lt;- steepestDescent(FUNC, GRAD, c(1,2), stepsize = 0.05)
MIN2 &lt;- steepestDescent(FUNC, GRAD, c(10,10), stepsize = 0.05)

# Optimised stepsize
MIN3 &lt;- steepestDescent(FUNC, GRAD, c(1,2), fixed.stepsize = F)
MIN4 &lt;- steepestDescent(FUNC, GRAD, c(10,10), fixed.stepsize = F)

MIN1[c(1,3)] # Local</code></pre>
<pre><code>## $minimum
## [1] -3.59132 -1.79566
## 
## $iterations
## [1] 304</code></pre>
<pre class="r"><code>MIN2[c(1,3)] # Global</code></pre>
<pre><code>## $minimum
## [1] 15.97712  7.98856
## 
## $iterations
## [1] 198</code></pre>
<pre class="r"><code>MIN3[c(1,3)] # Local</code></pre>
<pre><code>## $minimum
## [1] -3.59132 -1.79566
## 
## $iterations
## [1] 37</code></pre>
<pre class="r"><code>MIN4[c(1,3)] # Globa</code></pre>
<pre><code>## $minimum
## [1] 15.97712  7.98856
## 
## $iterations
## [1] 55</code></pre>
<div id="function-4-plots." class="section level2">
<h2>Function 4: Plots.</h2>
<p><img src="gradient_descent_files/figure-html/unnamed-chunk-10-1.png" width="672" style="display: block; margin: auto;" /></p>
<!-- ----------------------------------------------------------------------- -->
<div style="page-break-after: always;"></div>
<!-- ----------------------------------------------------------------------- -->
<p><img src="gradient_descent_files/figure-html/unnamed-chunk-11-1.png" width="50%" /><img src="gradient_descent_files/figure-html/unnamed-chunk-11-2.png" width="50%" /><img src="gradient_descent_files/figure-html/unnamed-chunk-11-3.png" width="50%" /><img src="gradient_descent_files/figure-html/unnamed-chunk-11-4.png" width="50%" /></p>
<!-- ----------------------------------------------------------------------- -->
<div style="page-break-after: always;"></div>
<!-- ----------------------------------------------------------------------- -->
</div>
</div>
<div id="function-5-rosenbrock-function." class="section level1">
<h1>Function 5: Rosenbrock Function.</h1>
<p>This function is used frequently in order to test the power of
optimisation algorithms since finding the global optimum of this
function can be quite challenging. Generally, it may not be hard for the
algorithm to determine the approximate region where the global minimum
is attained, yet converging to the exact global minimum can be
problematic.</p>
<p><span class="math display">\[
f(x, y) = (a-x)^2+b(y-x^2)^2
\]</span> The function has a global minimum at <span
class="math inline">\((x,y) = (a, a^2)\)</span> where <span
class="math inline">\(f(x,y) = 0\)</span>. We set the parameters to
<span class="math inline">\(a = 1, b = 100\)</span>, hence our global
minimum is at <span class="math inline">\((x, y) = (1,1)\)</span>.</p>
<pre class="r"><code>FUNC &lt;- function(x) {
  (1 - x[1])^2 + 100 * (x[2] - x[1]^2)^2
}

GRAD &lt;- function(x) {
  c(2 * (1-x[1]) - 400 * x[1] * (x[2] - x[1]^2), 
    200 * (x[2] - x[1]^2))
}

# Fixed stepsize
MIN1 &lt;- steepestDescent(FUNC, GRAD, c(2, 2), stepsize = 0.001)
MIN2 &lt;- steepestDescent(FUNC, GRAD, c(0.99, 0.99), stepsize = 0.001)

# Optimised stepsize
MIN3 &lt;- steepestDescent(FUNC, GRAD, c(2, 2), fixed.stepsize = F)
MIN4 &lt;- steepestDescent(FUNC, GRAD, c(0.99, 0.99), fixed.stepsize = F)

MIN1[c(1,3)] # Reached maximum iterations, not converging. </code></pre>
<pre><code>## $minimum
## [1] 1.528149 2.243863
## 
## $iterations
## [1] 10000</code></pre>
<pre class="r"><code>MIN2[c(1,3)] # Reached maximum iterations, not converging. </code></pre>
<pre><code>## $minimum
## [1] -0.4223657  0.1714116
## 
## $iterations
## [1] 10000</code></pre>
<pre class="r"><code>MIN3[c(1,3)] # Reached maximum iterations, not converging. </code></pre>
<pre><code>## $minimum
## [1] -1.968756  3.868919
## 
## $iterations
## [1] 10000</code></pre>
<pre class="r"><code>MIN4[c(1,3)] # Global</code></pre>
<pre><code>## $minimum
## [1] 0.9921088 0.9843116
## 
## $iterations
## [1] 10000</code></pre>
<p>While it does find some local stationary points, we see that the
algorithm fails entirely for this non-convex function when looking for
the global minimum. The algorithm continuously provides us with false
results. When considering the starting value <span
class="math inline">\(x_0 = (0.99, 0.99)\)</span> the initial guess is
in fact closer to the global minimum than the final result of the
algorithm.</p>
<div id="function-5-plots." class="section level2">
<h2>Function 5: Plots.</h2>
<p><img src="gradient_descent_files/figure-html/unnamed-chunk-13-1.png" width="672" style="display: block; margin: auto;" /></p>
<p><img src="gradient_descent_files/figure-html/unnamed-chunk-14-1.png" width="50%" /><img src="gradient_descent_files/figure-html/unnamed-chunk-14-2.png" width="50%" /><img src="gradient_descent_files/figure-html/unnamed-chunk-14-3.png" width="50%" /><img src="gradient_descent_files/figure-html/unnamed-chunk-14-4.png" width="50%" /></p>
<!-- ----------------------------------------------------------------------- -->
<div style="page-break-after: always;"></div>
<!-- ----------------------------------------------------------------------- -->
</div>
</div>
<div id="function-6-rastrigin-function." class="section level1">
<h1>Function 6: Rastrigin Function.</h1>
<p>This is a particularly difficult function to optimise, even in two
variables, as the plot below shows. There are many local minima, yet
only one global minimum (the origin).</p>
<p>The search domain is typically <span class="math inline">\(x_i \in
[-5.12, 5.12]\)</span>.</p>
<p><span class="math display">\[
f(\vec{x}) = An + \sum_{i=1}^n [x_i^2 - A\cos(2\pi x_i)]
\]</span></p>
<p>Where <span class="math inline">\(A=10\)</span> and <span
class="math inline">\(n=2\)</span>:</p>
<p><span class="math display">\[
f(x) = 20 + [x_1^2 - 10\cos(2\pi x_1)] + [x_2^2 - 10\cos(2\pi x_2)]
\]</span></p>
<p><span class="math display">\[
\text{Global minimum: } f(0,0) = 0 \\
\]</span></p>
<pre class="r"><code>FUNC &lt;- function(x) {
    20 + x[1]^2 - 10*cos(2*pi*x[1]) + x[2]^2 - 10*cos(2*pi*x[2])
}

GRAD &lt;- function(x) {
    c( 2*(x[1] + 10*pi*sin(2*pi*x[1])),
       2*(x[2] + 10*pi*sin(2*pi*x[2])))
}

# Fixed stepsize
MIN1 &lt;- steepestDescent(FUNC, GRAD, c(1.5,1.5), stepsize = 0.005)
MIN2 &lt;- steepestDescent(FUNC, GRAD, c(-0.35,-0.35), stepsize = 0.005)

# Optimised stepsize
MIN3 &lt;- steepestDescent(FUNC, GRAD, c(1.5,1.5), fixed.stepsize = F)
MIN4 &lt;- steepestDescent(FUNC, GRAD, c(-0.35,-0.35), fixed.stepsize = F)

MIN1[c(1,3)] # Local</code></pre>
<pre><code>## $minimum
## [1] 0.9949586 0.9949586
## 
## $iterations
## [1] 1163</code></pre>
<pre class="r"><code>MIN2[c(1,3)] # Global</code></pre>
<pre><code>## $minimum
## [1] 2.616849e-11 2.616849e-11
## 
## $iterations
## [1] 1293</code></pre>
<pre class="r"><code>MIN3[c(1,3)] # Local</code></pre>
<pre><code>## $minimum
## [1] 0.9949586 0.9949586
## 
## $iterations
## [1] 75</code></pre>
<pre class="r"><code>MIN4[c(1,3)] # Local</code></pre>
<pre><code>## $minimum
## [1] 0.9949586 0.9949586
## 
## $iterations
## [1] 13</code></pre>
<p>We see that the algorithm is unable to detect the correct global
minimum in most cases. This function provides us with a very interesting
insight. The same starting value lead to a correct result in the case of
a fixed stepsize, and an incorrect result for the optimal stepsize.</p>
<div id="function-6-plots." class="section level2">
<h2>Function 6: Plots.</h2>
<p><img src="gradient_descent_files/figure-html/unnamed-chunk-16-1.png" width="672" style="display: block; margin: auto;" /></p>
<p><img src="gradient_descent_files/figure-html/unnamed-chunk-17-1.png" width="50%" /><img src="gradient_descent_files/figure-html/unnamed-chunk-17-2.png" width="50%" /><img src="gradient_descent_files/figure-html/unnamed-chunk-17-3.png" width="50%" /><img src="gradient_descent_files/figure-html/unnamed-chunk-17-4.png" width="50%" /></p>
<!-- ----------------------------------------------------------------------- -->
<div style="page-break-after: always;"></div>
<!-- ----------------------------------------------------------------------- -->
</div>
</div>
<div id="function-7-himmelblaus-function." class="section level1">
<h1>Function 7: Himmelblau’s Function.</h1>
<p>This multi-modal function is defined as:</p>
<p><span class="math display">\[f(x, y) =
(x^2+y-11)^2+(x+y^2-7)^2\]</span></p>
<p>and has one local maximum at <span class="math inline">\((x,y) =
(-0.270845, -0.923039)\)</span> where the function value is <span
class="math inline">\(f(x,y) = 181.617\)</span>, and four identical
local minima:</p>
<ul>
<li>f(3,2) = 0</li>
<li>f(-2.805118, 3.131312) = 0</li>
<li>f(-3.779310, -3.283186) = 0</li>
<li>f(3.584428, -1.8481126) = 0</li>
</ul>
<p>This case is especially interesting as different starting values may
lead to different local minima.</p>
<pre class="r"><code>FUNC &lt;- function(x) {
  (x[1]^2 + x[2] - 11)^2 + (x[1] + x[2]^2 - 7)^2
}

GRAD &lt;- function(x) {
  c(4 * x[1] * (x[1]^2 + x[2] - 11) + 2 * (x[1] + x[2]^2 - 7), 
    2 * (x[1]^2 + x[2] - 11) + 4* x[2] * (x[1] + x[2]^2 - 7))
}

# Fixed stepsize
MIN1 &lt;- steepestDescent(FUNC, GRAD, c(0.5,0.5), stepsize = 0.005)
MIN2 &lt;- steepestDescent(FUNC, GRAD, c(-1,1), stepsize = 0.005)
MIN3 &lt;- steepestDescent(FUNC, GRAD, c(-0.5, -1), stepsize = 0.005)
MIN4 &lt;- steepestDescent(FUNC, GRAD, c(0.5,-2), stepsize = 0.005)


# Optimised stepsize
MIN5 &lt;- steepestDescent(FUNC, GRAD, c(0.5,0.5), fixed.stepsize = F)
MIN6 &lt;- steepestDescent(FUNC, GRAD, c(-1,1), fixed.stepsize = F)
MIN7 &lt;- steepestDescent(FUNC, GRAD, c(-0.5, -1), fixed.stepsize = F)
MIN8 &lt;- steepestDescent(FUNC, GRAD, c(0.5,-2), fixed.stepsize = F)


MIN1[c(1,3)] # correct</code></pre>
<pre><code>## $minimum
## [1] 3 2
## 
## $iterations
## [1] 151</code></pre>
<pre class="r"><code>MIN2[c(1,3)] # correct</code></pre>
<pre><code>## $minimum
## [1] -2.805118  3.131313
## 
## $iterations
## [1] 67</code></pre>
<pre class="r"><code>MIN3[c(1,3)] # correct</code></pre>
<pre><code>## $minimum
## [1] -3.779310 -3.283186
## 
## $iterations
## [1] 71</code></pre>
<pre class="r"><code>MIN4[c(1,3)] # correct</code></pre>
<pre><code>## $minimum
## [1]  3.584428 -1.848127
## 
## $iterations
## [1] 137</code></pre>
<pre class="r"><code>MIN5[c(1,3)] # correct</code></pre>
<pre><code>## $minimum
## [1] 3 2
## 
## $iterations
## [1] 34</code></pre>
<pre class="r"><code>MIN6[c(1,3)] # correct</code></pre>
<pre><code>## $minimum
## [1] -2.805118  3.131313
## 
## $iterations
## [1] 22</code></pre>
<pre class="r"><code>MIN7[c(1,3)] # correct</code></pre>
<pre><code>## $minimum
## [1] -3.779310 -3.283186
## 
## $iterations
## [1] 41</code></pre>
<pre class="r"><code>MIN8[c(1,3)] # correct</code></pre>
<pre><code>## $minimum
## [1]  3.584428 -1.848127
## 
## $iterations
## [1] 18</code></pre>
<div id="function-7-plots." class="section level2">
<h2>Function 7: Plots.</h2>
<p><img src="gradient_descent_files/figure-html/unnamed-chunk-19-1.png" width="672" style="display: block; margin: auto;" /></p>
<p><img src="gradient_descent_files/figure-html/unnamed-chunk-20-1.png" width="50%" /><img src="gradient_descent_files/figure-html/unnamed-chunk-20-2.png" width="50%" /><img src="gradient_descent_files/figure-html/unnamed-chunk-20-3.png" width="50%" /><img src="gradient_descent_files/figure-html/unnamed-chunk-20-4.png" width="50%" /><img src="gradient_descent_files/figure-html/unnamed-chunk-20-5.png" width="50%" /><img src="gradient_descent_files/figure-html/unnamed-chunk-20-6.png" width="50%" /><img src="gradient_descent_files/figure-html/unnamed-chunk-20-7.png" width="50%" /><img src="gradient_descent_files/figure-html/unnamed-chunk-20-8.png" width="50%" /></p>
<!-- ----------------------------------------------------------------------- -->
<div style="page-break-after: always;"></div>
<!-- ----------------------------------------------------------------------- -->
</div>
</div>
<div id="summary." class="section level1">
<h1>Summary.</h1>
<p>Summarizing we find that the algorithm works very well for most
functions, often times requiring only a few iterations until finding the
global optimum. The algorithm was successful in determining all four
local minima for the famous mutimodal Himmelblau’s Function.</p>
<p>The Rastrigin function required a very educated guess for the initial
point in order to find the global minimum due to there being so many
local minima in close proximity.</p>
<p>The Rosenbrock function was an exception, and would often lead to
misleading results due to the fact that big regions of the function are
so close to being flat, but not quite, that the function would determine
stationarity when in fact it was not a minimum.</p>
<p>Another observation was that using a <em>fixed</em> stepsize almost
always converged to a local or global minimum. However, sometimes in
testing, the use of the <em>optimal</em> stepsize sometimes returned an
initial stepsize that would overshoot and lead to a non-convergence of
the function, or overshoot a global minimum and return a local minimum
instead. Nevertheless, use of the optimal stepsize would generally
result in a much faster convergence as expected.</p>
</div>

Copyright &copy; 1995. All rights reserved.



</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
